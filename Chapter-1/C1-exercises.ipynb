{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises for Chapter 1 of Sutton & Barto (2nd edition)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Self-play_ Suppose, instead of playing against an opponent, a reinforcement learning algorithm plays against itself, with both sides learning. What do you think will happen in this case? Would it learn a different policy for selecting moves?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Solution**: In self-play, a reinforcement learning algorithm will play against itself, resulting in both sides learning from the same experiences. This process may lead to the discovery of a better policy as both sides continue to improve their play. As the agent learns and refines its strategy, it will adapt to exploit the weaknesses of its opponent (itself). The agent will keep learning from the mistakes made by both sides, eventually converging to an optimal or near-optimal strategy. However, the learned policy might be different from one obtained when playing against a fixed opponent or multiple opponents with diverse strategies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Symmetries_ Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described in this chapter to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To take advantage of symmetries, we can alter the learning process to treat symmetrically equivalent positions as the same. This can be done by storing and updating the value of only one representative state for each group of symmetrically equivalent states. Whenever the agent encounters a state, it would look up the value of the representative state for that group.\n",
    "\n",
    "This change would improve the learning process by reducing the state space, making it more efficient in terms of memory and computation. Additionally, it allows the agent to learn faster, as the learning from one state will automatically apply to all symmetrically equivalent states.\n",
    "\n",
    "If the opponent does not take advantage of symmetries, we should still consider them. The value of symmetrically equivalent positions may not be the same, as the opponent's response to each position might differ. However, incorporating symmetries can still help the agent learn faster and reduce the state space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Greedy play_ Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Solution**: A greedy reinforcement learning player will always play the move that it rates the best. This may lead to the discovery of a better policy as the agent continues to improve its play. However, the agent may also get stuck in a suboptimal policy, as it will not explore other moves that may lead to a better outcome. This problem is known as the _exploration-exploitation dilemma_."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Learning from exploration_ Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Solution**: If learning updates occur after all moves, including exploratory moves, the agent will learn from both its exploratory actions and its greedy actions. This means that the agent will be able to incorporate more diverse experiences into its learning process. As a result, the agent might learn a more accurate estimate of the state-action values and potentially discover better strategies.\n",
    "\n",
    "If the step-size parameter is appropriately reduced over time while maintaining exploration, the agent will continue to explore the state-action space but give more weight to recent experiences. This ensures that the agent doesn't get stuck in a suboptimal policy based on early experiences while still allowing for the possibility of finding better actions.\n",
    "\n",
    "However, it is important to note that updating the value estimates after exploratory moves can also introduce noise into the learning process. Since exploratory moves are not always optimal, the agent might incorporate suboptimal experiences into its estimates. This could slow down convergence to the optimal policy, but with a proper balance between exploration and exploitation, the agent can still learn effectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Other improvements_ Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
